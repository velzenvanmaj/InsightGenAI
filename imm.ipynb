{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Innovation Maturity Model (IMM)\n",
    "\n",
    "The **Innovation Maturity Model (IMM)** is a framework that helps organizations evaluate and improve their innovation capacity and processes. The model provides a structured approach to assessing an organization's innovation maturity by analyzing various dimensions of innovation, such as strategy, culture, processes, leadership, technology, and collaboration. The purpose of the model is to identify the organization's level of innovation maturity and outline the steps necessary to further enhance innovation and leverage it strategically.\n",
    "\n",
    "### Key Dimensions of the Innovation Maturity Model:\n",
    "\n",
    "- **Innovation Strategy**: Does the organization have a clear vision and strategy for innovation, and how broadly is it shared?\n",
    "- **Innovation Culture**: Is innovation actively promoted in the organizational culture, and are employees encouraged to introduce and experiment with new ideas?\n",
    "- **Leadership & Support**: To what extent does the organizationâ€™s leadership support and encourage innovation initiatives?\n",
    "- **Process and Structure**: Are there structured processes in place for developing, testing, and implementing new ideas?\n",
    "- **Use of Technology**: To what extent does the organization effectively utilize technology to enable and accelerate innovation?\n",
    "- **Knowledge Sharing and Collaboration**: How well is knowledge shared within teams and departments to foster innovation?\n",
    "- **Customer-Centric Innovation**: Are customer needs and feedback incorporated into the innovation process?\n",
    "- **Risk Management**: How does the organization manage the risks associated with innovation projects?\n",
    "- **Innovation Budget**: Is there sufficient budget allocated to support innovation?\n",
    "- **Speed of Innovation**: How quickly can the organization turn new ideas into realistic and executable projects?\n",
    "\n",
    "### Innovation Maturity Levels:\n",
    "\n",
    "The IMM distinguishes between various levels of maturity, often categorized into 4 or 5 stages, from \"beginner\" to \"innovation leader\":\n",
    "\n",
    "1. **Ad-hoc**: Innovation is sporadic and unstructured, with no clear process or strategy.\n",
    "2. **Repeatable**: Innovation occurs more frequently but remains largely dependent on individual initiatives and is not widespread.\n",
    "3. **Defined**: Innovation processes are established and consistently applied. There is broader involvement across teams and departments.\n",
    "4. **Managed**: Innovation is integrated into the business strategy, with clear KPIs and management processes in place to drive innovation.\n",
    "5. **Optimized**: The organization is an innovation leader, with continuous improvement supported by advanced technologies and a strong culture of creativity and collaboration.\n",
    "\n",
    "### Purpose of the IMM:\n",
    "\n",
    "The purpose of the IMM is to help organizations understand where they stand on this scale and identify actions to elevate their innovation capabilities. It serves not only as a diagnostic tool but also as a roadmap for strategic improvement.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### CONFIG SETTINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install numpy==1.23.5 langchain-openai==0.1.6 markdown2==2.4.13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import markdown2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from datetime import datetime, timedelta\n",
    "from langchain.chains import SequentialChain, LLMChain\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai.llms import OpenAI\n",
    "from langchain_core.prompts import SystemMessagePromptTemplate, HumanMessagePromptTemplate, ChatPromptTemplate, PromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### INITIALIZATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### SPECIFIC PARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = 'data'\n",
    "TEMPLATES_FOLDER = 'templates'\n",
    "QUESTIONNAIRE = \"imm_questionaire.csv\"\n",
    "report_name = \"Innovation-Scan Report.md\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### PATHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input paths.\n",
    "input_questionnaire_file_path = os.path.join(DATA_FOLDER, QUESTIONNAIRE)\n",
    "\n",
    "# Output paths.\n",
    "output_report_file_path = os.path.join(os.curdir, report_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### SPECIFIC FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_file(file_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Transform the CSV file by transposing the columns and convert it to JSON.\n",
    "    \"\"\"\n",
    "    # Read the CSV file.\n",
    "    df_full = pd.read_csv(file_path)\n",
    "\n",
    "    # The text column names.\n",
    "    column_names = df_full.columns[0:]\n",
    "\n",
    "    # The result.\n",
    "    result = {}\n",
    "\n",
    "    # Add the data for each question.\n",
    "    questions_data = []\n",
    "    for column in column_names:\n",
    "        question_data = df_full[column].tolist()\n",
    "        questions_data.append({\"vraag\": column, \"antwoord\": question_data})\n",
    "\n",
    "    result['vragen_en_antwoorden'] = questions_data\n",
    "\n",
    "    # Convert the result to JSON.\n",
    "    return json.dumps(result, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_template(template_name: str, section: str = None) -> str:\n",
    "    \"\"\"\n",
    "    Returns the contents of the specific template.\n",
    "    \"\"\"\n",
    "    if section is None:\n",
    "        template_file_path = os.path.join(os.path.join(TEMPLATES_FOLDER, f'{template_name}.md'))\n",
    "    else:\n",
    "        template_file_path = os.path.join(os.path.join(TEMPLATES_FOLDER, section), f'{template_name}.md')\n",
    "\n",
    "    # Load the data.\n",
    "    with open(template_file_path, 'r', encoding='utf-8') as file:\n",
    "        contents = file.read()\n",
    "\n",
    "    return contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_analysis(questionnaire: dict, system_template: str, human_template: str) -> str:\n",
    "    system_message_prompt = SystemMessagePromptTemplate.from_template(system_template)\n",
    "    human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
    "    chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
    "\n",
    "    request = chat_prompt.format_prompt(\n",
    "        questionnaire=json.dumps(questionnaire, ensure_ascii=False, indent=4),\n",
    "    ).to_messages()\n",
    "\n",
    "    # Define the LLM.\n",
    "    chat = ChatOpenAI(temperature=0.7, max_tokens=4096, model=\"gpt-4o-2024-05-13\", openai_api_key=OPENAI_API_KEY)\n",
    "    result = chat(request)\n",
    "\n",
    "    return result.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_recommendations(analysis: str, system_template: str, human_template: str) -> str:\n",
    "    system_message_prompt = SystemMessagePromptTemplate.from_template(system_template)\n",
    "    human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
    "    chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
    "\n",
    "    request = chat_prompt.format_prompt(\n",
    "        analysis=analysis,\n",
    "    ).to_messages()\n",
    "\n",
    "    # Define the LLM.\n",
    "    chat = ChatOpenAI(temperature=0.7, max_tokens=4096, model=\"gpt-4o-2024-05-13\", openai_api_key=OPENAI_API_KEY)\n",
    "    result = chat(request)\n",
    "\n",
    "    return result.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### MAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transpose the columns of the questionnaire.\n",
    "completed_questionnaire_content = transform_file(input_questionnaire_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis.\n",
    "analysis = generate_analysis(\n",
    "    questionnaire=completed_questionnaire_content,\n",
    "    system_template=get_template('system', 'analysis'),\n",
    "    human_template=get_template('human', 'analysis')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recommendations.\n",
    "recommendations = generate_recommendations(\n",
    "    analysis=analysis,\n",
    "    system_template=get_template('system', 'recommendations'),\n",
    "    human_template=get_template('human', 'recommendations')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the LLM.\n",
    "model = ChatOpenAI(temperature=0.7, max_tokens=4096, model=\"gpt-4o-2024-05-13\", openai_api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the initial check prompt.\n",
    "initial_check_prompt = PromptTemplate(\n",
    "    input_variables=[\n",
    "        \"analysis\",\n",
    "        \"recommendations\"\n",
    "    ],\n",
    "    template=get_template('prompt', 'check')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combined raw content.\n",
    "combined_raw_content = {\n",
    "    \"analysis\": analysis,\n",
    "    \"recommendations\": recommendations\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the initial check.\n",
    "initial_check_prompt_filled = initial_check_prompt.format(**combined_raw_content)\n",
    "initial_check = model.invoke(input=initial_check_prompt_filled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the prompts.\n",
    "consistency_prompt = PromptTemplate(input_variables=[\"section_content\", \"feedback\"], template=get_template('consistency', 'refinement'))\n",
    "coherence_prompt = PromptTemplate(input_variables=[\"consistency_content\", \"feedback\"], template=get_template('coherence', 'refinement'))\n",
    "language_prompt = PromptTemplate(input_variables=[\"coherence_content\", \"feedback\"], template=get_template('language', 'refinement'))\n",
    "fact_checking_prompt = PromptTemplate(input_variables=[\"language_content\", \"feedback\"], template=get_template('fact_checking', 'refinement'))\n",
    "clarity_readability_prompt = PromptTemplate(input_variables=[\"fact_checking_content\", \"feedback\"], template=get_template('clarity_readability', 'refinement'))\n",
    "relevance_conciseness_prompt = PromptTemplate(input_variables=[\"clarity_readability_content\", \"feedback\"], template=get_template('relevance_conciseness', 'refinement'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the LLM.\n",
    "llm = ChatOpenAI(temperature=0.7, max_tokens=4096, model=\"gpt-4o-2024-05-13\", openai_api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define individual chains for each check.\n",
    "consistency_chain = LLMChain(llm=llm, prompt=consistency_prompt, output_key=\"consistency_content\")\n",
    "coherence_chain = LLMChain(llm=llm, prompt=coherence_prompt, output_key=\"coherence_content\")\n",
    "language_chain = LLMChain(llm=llm, prompt=language_prompt, output_key=\"language_content\")\n",
    "fact_checking_chain = LLMChain(llm=llm, prompt=fact_checking_prompt, output_key=\"fact_checking_content\")\n",
    "clarity_readability_chain = LLMChain(llm=llm, prompt=clarity_readability_prompt, output_key=\"clarity_readability_content\")\n",
    "relevance_conciseness_chain = LLMChain(llm=llm, prompt=relevance_conciseness_prompt, output_key=\"refined_section\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine into a sequential chain for refinement.\n",
    "refinement_chain = SequentialChain(\n",
    "    chains=[consistency_chain, coherence_chain, language_chain, fact_checking_chain, clarity_readability_chain, relevance_conciseness_chain],\n",
    "    input_variables=[\"section_content\", \"feedback\"],\n",
    "    output_variables=[\"refined_section\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The refined content.\n",
    "refined_analysis = refinement_chain.run(section_content=analysis, feedback=initial_check)\n",
    "refined_recommendations = refinement_chain.run(section_content=recommendations, feedback=initial_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Innovation-Scan report.\n",
    "report = get_template('report').format(\n",
    "    analysis=refined_analysis,\n",
    "    recommendations=refined_recommendations\n",
    ")\n",
    "\n",
    "# Write the report.\n",
    "with open(output_report_file_path, 'w', encoding='utf-8') as file:\n",
    "    file.write(report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
